\mainmatter

# (PART) Part One {-}

# History & Why Reproducibility is Important {#history}

## Reproducible Research

So why is reproducible research important and incredibly beneficial? Let's take a look at its beginnings to find the answer. 

In the early '90s, a geophysicist named Jon Claerbout revised his book Earth Soundings Analysis with a valid complaint. He claimed that few published results are reproducible in any practical sense. To verify them requires almost as much effort as it took to create them in the first place. After a time, even the authors are often unable to reproduce their own results! For these reasons, many people ignore most of the literature.

Then, in 1996, the Consolidated Standards of Reporting Trials (or CONSORT) published a set of guidelines to fix problems that developed from inadequate reporting of randomized controlled trials. Following their lead, in 2004, the International Committee of Medical Journal Editors stated they wouldn't publish a clinical trial that had not been registered, and that they would only endorse registries meeting several key criteria, including that they must be: 

* free and publicly accessible, 
* open to all prospective registrants, 
* managed by a non-profit organization, and
* electronically searchable and validated

As a result of this turmoil in validity and research, the Food and Drug Administration got on board and required the registration of even more clinical trials. The Journal of Biostatistics also encouraged reproducible practices  of author submissions. They began marking accepted papers based on the standards of reproducibility that were followed. For example, papers marked with a 'D' marking meant the data on which the study is based is freely available. A 'C' marking means the authors' code is freely available. And an 'R' marking is the gold standard, meaning that not only are the data and code freely available, but the associate editor for reproducibility was also able to reproduce the same results as the paper. 

An example of an article given the highest designation for a fully reproducible article is one published in 2009 entitled “Air pollution and health in Scotland: a multicity study.” To see the article's marking, you have to download the PDF and look for the marking letter in a bold box at the top right. 

Most compelling, in the early 2000s, John Ioannidis published an article with the highest downloads in the history of the Public Library of Science. It was entitled "Why most published research findings are false." Despite multiple organizations attempting to fix this issue, the Open Science Collaboration revealed in 2015 that they were only able to reproduce or replicate between 30-50% of the results from more than 100 studies. Ziemann, et.al. in 2016 also found that 20% of papers published in leading genomics journals have supplementary data files containing erroneous gene name conversions due to Microsoft Excel default settings. This 20% is an average, and some journals have even higher rates.

But this isn't new. In 2011, Alsheikh-Ali [Al shake], et.al. assessed 500 research papers with unsettling results. Of these 500 papers sent to high-impact research journals, 30% were not subject to any data availability policy. The papers that adhered to the data availability instructions did so by publicly depositing only the specific data type as required, making a statement of willingness to share, or actually sharing all the primary data. Overall, only 47 papers (that's only 9%!) deposited full primary raw data online. 

In the last several years, reproducibility errors have been at the center of some major controversies. In 2010, a published cancer clinical trial at Duke University was tested by two MD Anderson researchers, Keith Baggerly and Kevin Coombs. They found numerous spreadsheet errors leading to misalignment and incorrect assignment of cancer treatment therapies. Because of this, four papers published by the Duke team were retracted, the Duke lead scientist resigned, and Duke shut down three other trials using these results, and many patients have pursued legal action. 

Another famous study, often called the “excel-error heard round the world” – was based on a paper by two well-known economists at Harvard, Kenneth Rogoff and Carmen Reinhart. In their paper “Growth in a Time of Debt”, the authors claimed that countries whose debt exceeds 90 percent of their annual gross domestic product experience slower growth than countries with lower debt — a figure that's been cited by many people in order to justify slashing government spending. But when Thomas Herndon, a 28-year-old economics graduate student at the University of Massachusetts tried to reproduce the results, he discovered a major formula error in the excel data spreadsheet. The original paper had excluded key data from the countries of Canada, New Zealand, and Australia — all countries that experienced solid growth during periods of high debt and thus undercut the conclusion that high debt forestalls growth. 

Due to these critical moments in the last few decades, reproducibility continues to grow as new policies are adopted and the practices are applied. But part of the benefit of being in this course is that you get to be part of the reproducible research movement!

## Literate Programming

In 1991, around the same time Jon Claerbout coined the term “reproducible research”, the computer scientist, Donald Knuth, introduced the concept of “literate programming.” The idea of literate programming is that software/computer programs are written in a language humans can understand rather than a language only machines can understand. In literate programming, computer code is embedded within the program's documentation as opposed to the documentation embedded within computer code; the code follows the structure of the documentation.

The program that Donald Knuth used to implement his idea of “literate programming” was called WEB, which he introduced in 1981. WEB linked the TeX typesetting or formatting system for creating documents with the Pascal computer programming language. WEB was one of the first systems to directly link documentation creation and typesetting with computer programming. Donald Knuth chose the name WEB because it implied a program of ideas pieced together from simple materials.

Since WEB was introduced, many other programs implementing literate programming have emerged. Here are a few to give you an idea of the variety available.

* CWEB also created by Donald Knuth with Silvio Levy which was adapted for the C and C++ compute language instead of Pascal
* Axiom developed by IBM
* Noweb
* Literate
* Funnel WEB
* Molly 
* Codnar
* Jupyter Notebook (formerly IPython Notebook) and
* R Notebooks

## Dynamic Documentation

So literate programming is an approach that moves away from writing computer programs in a high-level machine language and instead combines programming language with documentation language so that the program reads almost like an essay or a piece of literature. But what about dynamic documentation? Dynamic documentation allows for constant change and is a tool that provides up-to-date reports if certain components, such as data or analysis, change.

In 2002, Friederich Leisch, a statistics professor from the University of Natural Resources and Life Sciences in Vienna, released the SWEAVE program for dynamic documentation generation. Notably, SWEAVE allows R code to be embedded within LaTeX documents. LaTeX is  a more modern version of the TeX typesetting program used by Donald Knuth. The really exciting feature of literate programming and dynamic documentation is highlighted in what Friedrich Leisch says about SWEAVE: since the underlying computer code is wholly integrated within the document itself, anytime there are changes to the underlying data or analyses or code, the report itself is automatically updated ON THE FLY!

The next evolution of ideas for literate programming and dynamic documentation have emerged from the R programming and RStudio communities. In 2012, Yihui Xie (yeewhay she) released the R package called knitr. This package was inspired by SWEAVE, and thus combines R code with text typesetting for producing documents. Like SWEAVE, knitr works with LaTeX but it also works with rmarkdown, which uses simple text markup syntax based on the original “markdown” package. The primary “markdown” package was introduced by John Gruber in 2004 to make it easier to “markup” plain text files for generating HTML documents – ideally without having to learn HTML. The rmarkdown package you’ll use throughout this course is built upon John Gruber’s “markdown”.

Rmarkdown itself was fully released in 2014 and its original objective was creating documents for the internet by creating HTML formatted documents. However, the “rmarkdown” package also leverages Pandoc for creating an even wider array of documentation formats – including:

* The DOC format, as used by Microsoft WORD or Google Docs
* The ODT format used by Libre Office
* The PDF format 
* EPUB for electronic-books
* Slide shows using HTML5
* And the original TeX document formats and related TeX based slide formats like Beamer.

In this course, you won't interact directly with Pandoc, but it has been bundled with RStudio since 2015– so when you install RStudio, you'll also get the functionality of Pandoc. If you would like to learn more about Pandoc, you can visit their website at pandoc.org. Since Pandoc can convert many different document formats, it's often called the “swiss army knife” for document conversion. Pandoc is extremely versatile, allowing conversion between HTML web-based formats, word processor type formats, electronic publishing (or EPUB) formats,  presentation slide-based formats, publication layout formats, TeX based formats, and many others.

Ultimately, the RStudio Interactive Development Environment becomes our central “HUB” for combining the capabilities of:

* The great packages of “knitr” and “rmarkdown” 
* with the built-in functionality of Pandoc for document conversion
* plus the fantastic analysis and graphics capabilities of the R programming language.

From the RStudio interface, we can access all of this functionality and create documents on the fly in multiple formats for multiple end uses and products.

## Data

Data can be thought of as many different things. We often think of data as numbers or even short text in a spreadsheet. But more often than not, data is “unstructured." Unstructured data includes text, which could come from multiple sources, including not only reports and documents, but books, blogs, and websites. Other kinds of data could be:

* Images and artwork
* video and other media
* interview transcripts
* and any other "RAW" materials needed to complete your project.

Regardless of what kind of “data” you have, your data should be:

* high quality
* reviewed for completeness
* reviewed for mistakes and errors, and
* checked for changes or updates

## Organization & Workflow

Because your projects may involve a variety of dynamic data, how do you ensure your reproducible workflow is always efficient? There are several principles to follow. The first starts with organization. Each project you work on should have its own file storage organization structure. Each document, code, script, and product should have a specific purpose, and the versions of these files should all be tracked with a version control system without creating multiple copies of the files. 

Following this lecture, I've included a reading page with a helpful example of great organizational structure on Github.

File names should be:

* readable by the computer, easy to search, easy to sort (especially by date and author if needed)
* human readable with logical naming schemes and contain enough info so a human knows what is in the file and what the file is for
* and short enough to be reasonably manageable
* consider user-based access and security (partitioned by "need to know" [users with editing and write permissions versus users with read-only access]

Having an organizational structure for your project is a good idea even if your project only includes yourself, because:

* projects grow
* you may need to support numerous documents and files
* And relationships change and can become complex

No matter what kind of product you want to produce, there should also be instructions on how to use and combine the files in your project. Your documentation is another important component, and it should be clear and well-defined so it can be easily understood by team members at all levels. The documentation could also follow literate programming principles combining the code + text + figures in one document. 

Ideally, your final workflow will allow any changes and updates to be automatically incorporated into your final product. You should write code/scripts to automate:

* raw data to processed output
* creating and removing temporary files
* creating tables, figures and other components
* assembling the components into final documents, products,  and
* rendering documents into multiple-desired formats

Standardization is also a critical component. Your documentation, code, or templates might be used again in other projects and should be standardized for easier integration and efficiency.  You don’t want to reinvent any wheels if you can help it.

Finally, your files, documents, and code should be stored and shared in a centralized way. Cloud-based computing often provides centralized storage and sharing of your projects with your team members and external stakeholders. 

## Dissemination

Once your project is complete, you should disseminate your work. Why?

* To store and share your data and code. Odds are you will reuse something from this project in a future project.
* To fulfill expectations/requirements to disseminate your findings by the funding agency or publisher of your work
* To increase visibility - when you are listed as the source, you become, by default, THE subject matter expert!
* To increase the speed of collaboration for faster advancement of science and knowledge in your field, and finally
* To increase goodwill with the community and public

Some ways to disseminate your work using Cloud-based solutions are:

* Dropbox 
* Google drive 
* Github (better with version control and tracking) 

Other ways to disseminate may be through:

* Journals - articles, manuscripts
* Books
* Blogs/Websites
* RSS (Rich Site Summary) feeds – like news feeds
* Rpubs – which we will discuss and try out in future lessons in this course
* Other online book platforms such as Gitbook and Bookdown 

Some examples of data repositories are:

* GenBank 
* PDB 

In addition to Github, other data and code sharing repositories include:

* Bitbucket 
* Dryad 
* Figshare
* Zenodo 

A helpful article was published in 2013 in the journal PLOS Computational Biology entitled “Ten Simple Rules for Reproducible Computational Research.” While the article focused on applications in computational biology, the key principles they recommended still apply, and include:

* avoid manual steps
* use version control and tracking
* implement standardized formats
* store and track raw data
* organize your output – their list recommends a hierarchical organization
* link textual documentation to the results
* and make the work transparent by allowing public access to scripts, runs, and results

When considering standard practices, think about your own work:

* What do you want to automate?
* What could you re-use?
    - For example, code, files, formatting, graphics, logos, header, footer, boilerplate?
* What should you share with your team?
* What do you find yourself doing over and over?
    - correcting or reformatting?
* If you won the lottery today and left your job, what do you need to tell your replacement so that they can pick up where you left off and complete your current tasks?

The purpose of this course is to help you find the answers to these questions to improve your own workflow, teamwork, and efficiency!

## 538.com

A good example of an organization that follows reproducible principles is 538.com. They write and host stories and opinion pieces covering politics, economics, health, popular culture, and sports. The founder, Nate Silver, and the 538 team are best known for their political polling and forecasting during the United States Presidential and related elections since 2008.

Most of their articles provide references and links to their original data sources, and they also host their data, code, and details behind their analyses on their Github, which is available to the public. We’re going to work with some of these datasets later in this course using the “fivethirtyeight” R package.

It's also worth mentioning Andrew Flowers, one of the contributors to the 'fivethirtyeight' R Package. He gave a great presentation at the 2017 RStudio conference on how to tell stories using data, and he highlighted the various aspects of "data journalism" and importance of workflow, data processing, and transparency in analysis and communication. These are all key aspects of reproducibility. 

## Saving Lives

To really see the power and importance of reproducible workflow principles, let's go back in history to 2001 where an outbreak of a deadly strain of e.coli bacteria killed 50 people in Europe. Researchers at the Beijing Genomics Institute worked in collaboration with the Medical Center in Hamburg-Eppendorf to rapidly sequence the genome of the e.coli pathogen. Given the severity of the outbreak, the team announced and released the genome via Twitter to the world-wide community of microbial genomicists.  A Github repository was established to "crowdsource" analysis and research to find a treatment.

People started contributing their work in under 24 HOURS, and within 5 DAYS a bacterial agent was proposed to kill the pathogen. This case highlights the importance of these methods and work practices not only for speed and efficiency but also for rapidly addressing problems and developing solutions to save lives.


